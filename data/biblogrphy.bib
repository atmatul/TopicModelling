@misc{,
mendeley-groups = {i5 - literature search},
title = {{Chapter 1: Introduction to Data Mining}},
url = {http://webdocs.cs.ualberta.ca/{~}zaiane/courses/cmput690/notes/Chapter1/index.html},
urldate = {2017-11-17}
}
@article{Tan,
author = {Tan, Ying and Takagi, Hideyuki and Shi, Yuhui},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Takagi, Shi - Unknown - Data Mining and Big Data.pdf:pdf},
journal = {LNCS Proceedings},
mendeley-groups = {i5 - literature search},
title = {{Data Mining and Big Data}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-61845-6.pdf}
}
@article{Walker2015,
abstract = {This paper presents Personnal Data Lake, a unified storage facility for storing, analyzing and querying personal data. A data lake stores data regardless of format and thus provides an intuitive way to store personal data fragments of any type. Metadata management is a central part lake architecture. For structured/semi-structured data fragments, metadata may contain information about the shema of the data so that the data can be transformed into queryable data objects when required. For unstructured data, enabling gravity pull means allowing third-party plugins so that the unstructured data can be analyzed and queried.},
author = {Walker, Coral and Alrehamy, Hassan},
doi = {10.1109/BDCloud.2015.62},
file = {:D$\backslash$:/D Drive/Thesis/Documents/Research Papers/Personal Data Lake With Data Gravity Pull.pdf:pdf},
isbn = {9781467371834},
journal = {Proceedings - 2015 IEEE 5th International Conference on Big Data and Cloud Computing, BDCloud 2015},
keywords = {big data,data lake,metadata,personal data},
mendeley-groups = {i5 - literature search},
pages = {160--167},
title = {{Personal Data Lake with Data Gravity Pull}},
year = {2015}
}
@article{Lake,
author = {Lake, Data},
file = {:D$\backslash$:/D Drive/Thesis/Documents/Research Papers/ER2016-phd-hai.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Towards an Intelligent Data Lake System for Heterogeneous Data Sources}}
}
@article{Boci2015,
abstract = {The first building block of the Federal Aviation Administration's (FAA) Next Generation Air Transportation System (NextGen) initiative to modernize the US national airspace system (NAS) was the implementation of the Automatic Dependent Surveillance-Broadcast (ADS-B) ground infrastructure. A primary aspect of the ADS-B program design is the terrestrial radio station infrastructure. It determined the terrestrial radio stations layout throughout the US and was optimized to meet system performance, safety and security in the NAS. In March 2014, the FAA completed the nationwide infrastructure upgrade, enabling air traffic controllers to track aircraft with greater accuracy and reliability, while giving pilots more information in the cockpit. More than 650 ADS-B radios communicate with equipped aircraft, supporting the new satellite-based surveillance system. Currently, the ADS-B system ingests processes and stores large data sets, while operating at ten percent capacity. As aircraft avionics equipage increases, the volume of data and storage needs will increase beyond our existing system's capacity and processing capability. A new, Hadoop-based architecture was tested to ingest and analyze billions of CAT033 reports in minutes. This paper presents the 'Big Data' approach that was adopted to support fast analytics of large ADS-B data volume. {\textcopyright} 2015 IEEE.},
author = {Boci, E and Thistlethwaite, S},
doi = {10.1109/ICNSURV.2015.7121218},
file = {:D$\backslash$:/D Drive/Thesis/Documents/Research Papers/A NOVEL BIG DATA ARCHITECTURE IN SUPPORT OF ADS-B DATA.pdf:pdf},
isbn = {9781479989522 (ISBN)},
journal = {2015 15th Annual Integrated Communication, Navigation and Surveillance Conference, ICNS 2015},
keywords = {Air traffic control,Air traffic controller,Air transportation,Aircraft detection,Automatic dependent surveillance - broadcasts,Balloons,Big data,Cockpits (aircraft),Deceleration,Digital storage,Federal Aviation Administration,Fighter aircraft,Ground infrastructures,Monitoring,National airspace system,Next-generation air transportation systems,Processing capability,Radio stations,Station infrastructure},
mendeley-groups = {i5 - literature search},
pages = {C11--C18},
title = {{A novel big data architecture in support of ADS-B data analytic}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84938778052{\&}partnerID=40{\&}md5=6329c29e6d5de8a7c7aa0c6fcaff5247},
year = {2015}
}
@misc{,
keywords = {gartner},
mendeley-groups = {i5 - literature search},
title = {{Data Lake - Gartner IT Glossary}},
url = {https://www.gartner.com/it-glossary/data-lake},
urldate = {2017-11-14}
}
@article{Dean,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dean, Ghemawat - Unknown - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{MapReduce: Simplified Data Processing on Large Clusters}},
url = {http://delivery.acm.org/10.1145/1330000/1327492/p107-dean.pdf?ip=137.226.149.66{\&}id=1327492{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=1005501679{\&}CFTOKEN=72168757{\&}{\_}{\_}acm{\_}{\_}=1510594534{\_}7acb6ec7c9d0b9dccb407}
}
@article{Quix,
author = {Quix, Christoph},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quix - Unknown - Data Lakes A Solution or a new Challenge for Big Data Integration Frequent Problems of a Big Data Project.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Data Lakes: A Solution or a new Challenge for Big Data Integration Frequent Problems of a Big Data Project}},
url = {http://dbis.rwth-aachen.de/{~}quix/papers/data2016.pdf}
}
@article{Chessell,
author = {Chessell, Mandy and Scheepers, Ferd and Nguyen, Nhan and Kessel, Ruud Van and {Van Der Starre}, Ron},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chessell et al. - Unknown - Governing and Managing Big Data for Analytics and Decision Makers.pdf:pdf},
keywords = {IBM Redbooks CICS InfoSphere Redguide WebSphere},
mendeley-groups = {i5 - literature search},
title = {{Governing and Managing Big Data for Analytics and Decision Makers}},
url = {http://www.redbooks.ibm.com/redpapers/pdfs/redp5120.pdf}
}
@article{Abadi,
abstract = {Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14-15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.},
author = {Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A and Carey, Michael J and Chaudhuri, Surajit and Dean, Jeffrey and Doan, Anhai and Franklin, Michael J and Gehrke, Johannes and Haas, Laura M and Halevy, Alon Y and Hellerstein, Joseph M and Ioannidis, Yannis E and Jagadish, H V and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R{\'{e}}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - The Beckman Report on Database Research.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{The Beckman Report on Database Research}},
url = {https://beckman.cs.wisc.edu/beckman-report2013.pdf}
}
@article{Stein2014,
abstract = {www.pwc.com/technologyforecast Data lakes that can scale at the pace of the cloud remove integration barriers and clear a path for more timely and informed business decisions. 2 PwC Technology Forecast The enterprise data lake: Better integration and deeper analytics Data lakes: An emerging approach to cloud-based big data UC Irvine Medical Center maintains millions of records for more than a million patients, including radiology images and other semi-structured reports, unstructured physicians' notes, plus volumes of spreadsheet data. To solve the challenge the hospital faced with data storage, integration, and accessibility, the hospital created a data lake based on a Hadoop architecture, which enables distributed big data processing by using broadly accepted open software standards and massively parallel commodity hardware. Hadoop allows the hospital's disparate records to be stored in their native formats for later parsing, rather than forcing all-or-nothing integration up front as in a data warehousing scenario. Preserving the native format also helps maintain data provenance and fidelity, so different analyses can be performed using different contexts. The data lake has made possible several data analysis projects, including the ability to predict the likelihood of readmissions and take preventive measures to reduce the number of readmissions. 1 Like the hospital, enterprises across industries are starting to extract and place data for analytics into a single Hadoop-based repository without first transforming the data the way they would need to for a relational data warehouse. 2 The basic concepts behind Hadoop 3 were devised by Google to meet its need for a flexible, cost-effective data processing model that could scale as data volumes grew faster than ever. Yahoo, Facebook, Netflix, and others whose business models also are based on managing enormous data volumes quickly adopted similar methods. Costs were certainly a},
author = {Stein, Brian and Morrison, Alan},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stein, Morrison - 2014 - The enterprise data lake Better integration and deeper analytics.pdf:pdf},
journal = {Technology Forecast: Rethinking integration Issue},
mendeley-groups = {i5 - literature search},
title = {{The enterprise data lake: Better integration and deeper analytics}},
url = {http://hortonworks.com/customer/uc-irvine-health/.},
volume = {1},
year = {2014}
}
@misc{Woods,
author = {Woods, Dan},
mendeley-groups = {i5 - literature search},
title = {{Big Data Requires a Big, New Architecture}},
url = {https://www.forbes.com/sites/ciocentral/2011/07/21/big-data-requires-a-big-new-architecture/{\#}3fe54ad41157},
urldate = {2017-11-06}
}
@misc{,
mendeley-groups = {i5 - literature search},
title = {{Deep Learning without Backpropagation - i am trask}},
url = {https://iamtrask.github.io/2017/03/21/synthetic-gradients/},
urldate = {2017-10-29}
}
@article{Li,
abstract = {One step in interoperating among heterogeneous databases is semantic integration: Identifying relationships between attributes or classes in dierent database schemas. SEMantic INTegrator (SEMINT) is a tool based on neural networks to assist in identifying attribute correspondences in heterogeneous databases. SEMINT supports access to a variety of database systems and utilizes both schema information and data contents to produce rules for matching corresponding attributes automatically. This paper provides theoretical background and implementation details of SEMINT. Ex-perimental results from large and complex real databases are presented. We discuss the eectiveness of SEMINT and our experiences with attribute correspondence identi{\textregistered}cation in various environments. {\'{O}}},
author = {Li, Wen-Syan and Clifton, Chris},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Clifton - Unknown - SEMINT A tool for identifying attribute correspondences in heterogeneous databases using neural networks.pdf:pdf},
keywords = {Attribute correspondence identi{\textregistered}cation,Database integration,Heterogeneous databases,Neural networks},
mendeley-groups = {i5 - literature search},
title = {{SEMINT: A tool for identifying attribute correspondences in heterogeneous databases using neural networks}},
url = {https://pdfs.semanticscholar.org/3fb2/e37d5d18767302c67f05795749079e106e0a.pdf}
}
@article{Rahm2001,
abstract = {Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically per-formed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match op-eration for specific application domains. We present a taxon-omy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distin-guish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some pre-vious match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different ap-proaches to schema matching, when developing a new match algorithm, and when implementing a schema matching com-ponent.},
author = {Rahm, Erhard and Bernstein, Philip A},
doi = {10.1007/s007780100057},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahm, Bernstein - 2001 - A survey of approaches to automatic schema matching.pdf:pdf},
journal = {The VLDB Journal},
keywords = {Graph matching –,Machine learning,Model management –,Schema integration –,Schema matching –},
mendeley-groups = {i5 - literature search},
pages = {334--350},
title = {{A survey of approaches to automatic schema matching}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs007780100057.pdf},
volume = {10},
year = {2001}
}
@misc{Madhavan,
abstract = {Schema Matching is the problem of identifying corresponding
elements in different schemas. Discovering these
correspondences or matches is inherently difficult to automate.
Past solutions have proposed a principled combination
of multiple algorithms. However, these solutions
sometimes perform rather poorly due to the lack of suffi-
cient evidence in the schemas being matched. In this paper
we show how a corpus of schemas and mappings can
be used to augment the evidence about the schemas being
matched, so they can be matched better. Such a corpus typically
contains multiple schemas that model similar concepts
and hence enables us to learn variations in the elements
and their properties. We exploit such a corpus in two
ways. First, we increase the evidence about each element
being matched by including evidence from similar elements
in the corpus. Second, we learn statistics about elements
and their relationships and use them to infer constraints
that we use to prune candidate mappings. We also describe
how to use known mappings to learn the importance of domain
and generic constraints. We present experimental results
that demonstrate corpus-based matching outperforms
direct matching (without the benefit of a corpus) in multiple
domains.},
author = {Madhavan, Jayant and Bernstein, Philip A.},
mendeley-groups = {i5 - literature search},
title = {{Corpus-based Schema Matching}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410106},
urldate = {2017-10-18}
}
@article{Doan2003,
abstract = {The problem of integrating data from multiple data sources—either on the Internet or within enter-prises—has received much attention in the database and AI communities. The focus has been on building data integration systems that provide a uniform query interface to the sources. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the query interface and the source schemas. Examples of mappings are " element location maps to address " and " price maps to listed-price " . We propose a multistrategy learning approach to automatically find such mappings. The approach applies multiple learner modules, where each module exploits a different type of information either in the schemas of the sources or in their data, then combines the predictions of the modules using a meta-learner. Learner modules employ a variety of techniques, ranging from Naive Bayes and nearest-neighbor classification to entity recognition and information retrieval. We describe the LSD system, which employs this approach to find semantic mappings. To further improve matching accuracy, LSD exploits domain integrity constraints, user feedback, and nested structures in XML data. We test LSD experimentally on several real-world domains. The experiments validate the utility of multistrategy learning for data integration and show that LSD proposes semantic mappings with a high degree of accuracy.},
author = {Doan, Anhai and Domingos, Pedro and Halevy, Alon},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doan, Domingos, Halevy - 2003 - Learning to Match the Schemas of Data Sources A Multistrategy Approach.pdf:pdf},
journal = {Machine Learning},
keywords = {data integration,multistrategy learning,schema matching},
mendeley-groups = {i5 - literature search},
pages = {279--301},
title = {{Learning to Match the Schemas of Data Sources: A Multistrategy Approach}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1021765902788.pdf},
volume = {50},
year = {2003}
}
@article{Doan,
abstract = {A data-integration system provides access to a multitude
of data sources through a single mediated schema. A key
bottleneck in building such systems has been the labori- ous manual construction of semantic mappings between the
source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning
techniques to semi-automatically nd such mappings. LSD
rst asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together
with the sources to train a set of learners. Each
learner exploits a dierent type of information either in the
source schemas or in their data. Once the learners have been
trained, LSD nds semantic mappings for a new data source
by applying the learners, then combining their predictions
using a meta-learner. To further improve matching accu- racy, we extend machine learning techniques so that LSD
can incorporate domain constraints as an additional source
of knowledge, and develop a novel learner that utilizes the
structural information in XML documents. Our approach
thus is distinguished in that it incorporates multiple types
of knowledge. Importantly, its architecture is extensible to
additional learners that may exploit new kinds of information.
We describe a set of experiments on several real-world
domains, and show that LSD proposes semantic mappings
with a high degree of accuracy},
author = {Doan, AnHai and Domingos, Pedro},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doan, Domingos, Halevy - 2001 - Reconciling Schemas of Disparate Data Sources A Machine-Learning Approach.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Reconciling Schemas of Disparate Data Sources: A Machine-Learning Approach}},
url = {http://delivery.acm.org/10.1145/380000/375731/p509-doan.pdf?ip=137.226.149.66{\&}id=375731{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.42406507AF320856.4D4702B0C3E38B35{\&}CFID=992624815{\&}CFTOKEN=52361782{\&}{\_}{\_}acm{\_}{\_}=1508335716{\_}21fe20e4414f755cc}
}
@article{Spaccapietra,
author = {Spaccapietra, Stefano},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Spaccapietra - Unknown - LNCS 3730 - Journal on Data Semantics IV.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{LNCS 3730 - Journal on Data Semantics IV}},
url = {https://link.springer.com/content/pdf/10.1007/11603412.pdf{\#}page=157}
}
@article{Persson,
author = {Persson, Anne and Stirna, Janis},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Persson, Stirna - Unknown - LNCS 3084 - Advanced Information Systems Engineering.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{LNCS 3084 - Advanced Information Systems Engineering}},
url = {https://link.springer.com/content/pdf/10.1007/b98058.pdf{\#}page=97}
}
@article{Lenzerini,
abstract = {Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integra-tion systems is important in current real world applications, and is characterized by a number of issues that are interest-ing from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the the-oretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and rea-soning on queries.},
author = {Lenzerini, Maurizio},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenzerini - Unknown - Data Integration A Theoretical Perspective.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Data Integration: A Theoretical Perspective}},
url = {http://delivery.acm.org/10.1145/550000/543644/p233-lenzerini.pdf?ip=137.226.149.66{\&}id=543644{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.42406507AF320856.4D4702B0C3E38B35{\&}CFID=992502267{\&}CFTOKEN=47515411{\&}{\_}{\_}acm{\_}{\_}=1507366648{\_}347a6cc1855fc7d900f7}
}
@article{Doan2001,
abstract = {A data-integration system provides access to a multitude
of data sources through a single mediated schema. A key
bottleneck in building such systems has been the labori- ous manual construction of semantic mappings between the
source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning
techniques to semi-automatically nd such mappings. LSD
rst asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together
with the sources to train a set of learners. Each
learner exploits a dierent type of information either in the
source schemas or in their data. Once the learners have been
trained, LSD nds semantic mappings for a new data source
by applying the learners, then combining their predictions
using a meta-learner. To further improve matching accu- racy, we extend machine learning techniques so that LSD
can incorporate domain constraints as an additional source
of knowledge, and develop a novel learner that utilizes the
structural information in XML documents. Our approach
thus is distinguished in that it incorporates multiple types
of knowledge. Importantly, its architecture is extensible to
additional learners that may exploit new kinds of information.
We describe a set of experiments on several real-world
domains, and show that LSD proposes semantic mappings
with a high degree of accuracy.},
author = {Doan, AnHai and Domingos, Pedro and Halevy, Alon},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doan, Domingos, Halevy - 2001 - Reconciling Schemas of Disparate Data Sources A Machine-Learning Approach.pdf:pdf},
journal = {SIGMOD '01 Proceedings of the 2001 ACM SIGMOD international conference on Management of data},
mendeley-groups = {i5 - literature search},
title = {{Reconciling Schemas of Disparate Data Sources: A Machine-Learning Approach}},
url = {http://delivery.acm.org/10.1145/380000/375731/p509-doan.pdf?ip=137.226.149.66{\&}id=375731{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=992502267{\&}CFTOKEN=47515411{\&}{\_}{\_}acm{\_}{\_}=1507321951{\_}eefe86a5a9c54323c},
year = {2001}
}
@article{Dhamankar,
abstract = {Creating semantic matches between disparate data sources is fundamental to numerous data sharing efforts. Manu-ally creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. To date, however, virtually all of these works deal only with one-to-one (1-1) matches, such as ad-dress = location. They do not consider the important class of more complex matches, such as address = concat(city,state) and room-price = room-rate * (1 + tax-rate). We describe the iMAP system which semi-automatically discovers both 1-1 and complex matches. iMAP reformu-lates schema matching as a search in an often very large or infinite match space. To search effectively, it employs a set of searchers, each discovering specific types of complex matches. To further improve matching accuracy, iMAP ex-ploits a variety of domain knowledge, including past complex matches, domain integrity constraints, and overlap data. Fi-nally, iMAP introduces a novel feature that generates ex-planation of predicted matches, to provide insights into the matching process and suggest actions to converge on correct matches quickly. We apply iMAP to several real-world do-mains to match relational tables, and show that it discovers both 1-1 and complex matches with high accuracy.},
author = {Dhamankar, Robin and Lee, Yoonkyong and Doan, Anhai and Halevy, Alon and Domingos, Pedro},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhamankar et al. - Unknown - iMAP Discovering Complex Semantic Matches between Database Schemas.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{iMAP: Discovering Complex Semantic Matches between Database Schemas}},
url = {http://delivery.acm.org/10.1145/1010000/1007612/p383-dhamankar.pdf?ip=137.226.149.66{\&}id=1007612{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=992502267{\&}CFTOKEN=47515411{\&}{\_}{\_}acm{\_}{\_}=1507321932{\_}61e4114de10cfd117}
}
@article{Satuluri,
abstract = {Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. Locality-sensitive hashing (LSH) based methods have become a very popular approach for this problem. However, most such methods only use LSH for the first phase of similarity search -i.e. efficient in-dexing for candidate generation. In this paper, we present BayesLSH, a principled Bayesian algorithm for the sub-sequent phase of similarity search -performing candidate pruning and similarity estimation using LSH. A simpler vari-ant, BayesLSH-Lite, which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and re-call. Finally, the quality of BayesLSH's output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2x-20x for a wide variety of datasets.},
author = {Satuluri, Venu and Parthasarathy, Srinivasan},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Satuluri, Parthasarathy - Unknown - Bayesian Locality Sensitive Hashing for Fast Similarity Search.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Bayesian Locality Sensitive Hashing for Fast Similarity Search}},
url = {http://vldb.org/pvldb/vol5/p430{\_}venusatuluri{\_}vldb2012.pdf}
}
@article{Das,
abstract = {We consider the problem of finding related tables in a large corpus of heterogenous tables. Detecting related tables pro-vides users a powerful tool for enhancing their tables with additional data and enables effective reuse of available pub-lic data. Our first contribution is a framework that captures several types of relatedness, including tables that are can-didates for joins and tables that are candidates for union. Our second contribution is a set of algorithms for detect-ing related tables that can be either unioned or joined. We describe a set of experiments that demonstrate that our al-gorithms produce highly related tables. We also show that we can often improve the results of table search by pulling up tables that are ranked much lower based on their related-ness to top-ranked tables. Finally, we describe how to scale up our algorithms and show the results of running it on a corpus of over a million tables extracted from Wikipedia.},
author = {Das, Anish and {\"{I}}¿¿, Sarma and Fang, Lujun and Gupta, Nitin and Halevy, Alon and Lee, Hongrae and Wu, Fei and Xin, Reynold and Yu, Cong},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - Unknown - Finding Related Tables.pdf:pdf},
keywords = {Design,General General Terms Algorithms,Management,Performance Keywords web tables,data integration,related tables},
mendeley-groups = {i5 - literature search},
title = {{Finding Related Tables}},
url = {http://i.stanford.edu/{~}anishds/publications/sigmod12/modi255i-dassarma.pdf}
}
@article{Ganti,
abstract = {Data warehouses consolidate various activities of a business and often form the backbone for generating reports that support important business decisions. Errors in data tend to creep in fora variety of reasons. Some of these reasons include errors during input data collection and errors while merging data collected independently across different databases. These errors in data warehouses often result in erroneous upstream reports, and could impact business decisions negatively. Therefore, one of the critical challenges while maintaining large data warehouses is that of ensuring the quality of data in the data warehouse remains high. The process of maintaining high data quality is commonly referred to as data cleaning. In this book, we first discuss the goals of data cleaning. Often, the goals of data cleaning are not well defined and could mean different solutions in different scenarios. Toward clarifying these goals, we abstract out a common set of data cleaning tasks that often need to be addressed. This abstraction allows us to develop solutions for these common data cleaning tasks. We then discuss a few popular approaches for developing such solutions. In particular, we focus on an operator-centric approach for developing a data cleaning platform. The operator-centric approach involves the development of customizable operators that could be used as building blocks for developing common solutions. This is similar to the approach of relational algebra for query processing. The basic set of operators can be put together to build complex queries. Finally, we discuss the development of custom scripts which leverage the basic data cleaning operators along with relational operators to implement effective solutions for data cleaning tasks. {\&} w w w . m o r g a n c l a y p o o l . c o m Data warehouses consolidate various activities of a business and often form the backbone for generating reports that support important business decisions. Errors in data tend to creep in fora variety of reasons. Some of these reasons include errors during input data collection and errors while merging data collected independently across different databases. These errors in data warehouses often result in erroneous upstream reports, and could impact business decisions negatively. Therefore, one of the critical challenges while maintaining large data warehouses is that of ensuring the quality of data in the data warehouse remains high. The process of maintaining high data quality is commonly referred to as data cleaning. In this book, we first discuss the goals of data cleaning. Often, the goals of data cleaning are not well defined and could mean different solutions in different scenarios. Toward clarifying these goals, we abstract out a common set of data cleaning tasks that often need to be addressed. This abstraction allows us to develop solutions for these common data cleaning tasks. We then discuss a few popular approaches for developing such solutions. In particular, we focus on an operator-centric approach for developing a data cleaning platform. The operator-centric approach involves the development of customizable operators that could be used as building blocks for developing common solutions. This is similar to the approach of relational algebra for query processing. The basic set of operators can be put together to build complex queries. Finally, we discuss the development of custom scripts which leverage the basic data cleaning operators along with relational operators to implement effective solutions for data cleaning tasks.},
author = {Ganti, Venkatesh and Sarma, Anish Das},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganti, Sarma - Unknown - Mor gan Cl aypool Publishers {\&}amp SYNTHESIS LECTURES ON DATA MANAGEMENT SYNTHESIS LECTURES ON DATA MANAGEMENT D.pdf:pdf},
isbn = {978-1-60845-677-2},
mendeley-groups = {i5 - literature search},
title = {{Mor gan Cl aypool Publishers {\&} SYNTHESIS LECTURES ON DATA MANAGEMENT SYNTHESIS LECTURES ON DATA MANAGEMENT Data Cleaning A Practical Perspective SYNTHESIS LECTURES ON DATA MANAGEMENT SYNTHESIS LECTURES ON DATA MANAGEMENT Data Cleaning A Practical Pers}},
url = {http://www.odbms.org/wp-content/uploads/2014/03/Data-Cleaning.pdf}
}
@article{Hassanzadeh,
abstract = {A basic step in integration is the identification of linkage points, i.e., finding attributes that are shared (or related) between data sources, and that can be used to match records or entities across sources. This is usually performed using a match operator, that associates attributes of one database to another. However, the mas-sive growth in the amount and variety of unstructured and semi-structured data on the Web has created new challenges for this task. Such data sources often do not have a fixed pre-defined schema and contain large numbers of diverse attributes. Furthermore, the end goal is not schema alignment as these schemas may be too hetero-geneous (and dynamic) to meaningfully align. Rather, the goal is to align any overlapping data shared by these sources. We will show that even attributes with different meanings (that would not qual-ify as schema matches) can sometimes be useful in aligning data. The solution we propose in this paper replaces the basic schema-matching step with a more complex instance-based schema analy-sis and linkage discovery. We present a framework consisting of a library of efficient lexical analyzers and similarity functions, and a set of search algorithms for effective and efficient identification of linkage points over Web data. We experimentally evaluate the effectiveness of our proposed algorithms in real-world integration scenarios in several domains.},
author = {Hassanzadeh, Oktie and {Pu UOIT}, Ken Q and {Hassas Yeganeh}, Soheil and ee Miller, Re J and Popa, Lucian and Her andez, Mauricio A and Ho, Howard},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassanzadeh et al. - Unknown - Discovering Linkage Points over Web Data.pdf:pdf},
keywords = {Data Integration,Entity Resolution,H25 [Database Man-agement],Heterogeneous Databases Keywords Record Linkage,Link Discovery,Schema Matching,Systems},
mendeley-groups = {i5 - literature search},
title = {{Discovering Linkage Points over Web Data}},
url = {http://www.vldb.org/pvldb/vol6/p445-hassanzadeh.pdf}
}
@incollection{Quix2017,
author = {Quix, Christoph},
booktitle = {Scientific Data Management},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quix - 2017 - Data Integration.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Data Integration}},
url = {http://hdl.handle.net/2022/13996},
year = {2017}
}
@article{Serafi,
author = {Serafi, Ayman El and Abell{\'{o}}, Alberto and Romero, Oscar},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Serafi, Abell{\'{o}}, Romero - Unknown - Information Profiling in the Data Lake – Using Data Mining techniques.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Information Profiling in the Data Lake – Using Data Mining techniques}},
url = {http://cs.ulb.ac.be/conferences/ebiss2017/files/slides/al-serafi{\_}ebiss2017}
}
@article{Wang,
abstract = {Document stores that provide the efficiency of a schema-less in-terface are widely used by developers in mobile and cloud appli-cations. However, the simplicity developers achieved controver-sially leads to complexity for data management due to lack of a schema. In this paper, we present a schema management frame-work for document stores. This framework discovers and persists schemas of JSON records in a repository, and also supports queries and schema summarization. The major technical challenge comes from varied structures of records caused by the schema-less data model and schema evolution. In the discovery phase, we apply a canonical form based method and propose an algorithm based on equivalent sub-trees to group equivalent schemas efficiently. To-gether with the algorithm, we propose a new data structure, eSiBu-Tree, to store schemas and support queries. In order to present a single summarized representation for heterogenous schemas in records, we introduce the concept of " skeleton " , and propose to use it as a relaxed form of the schema, which captures a small set of core attributes. Finally, extensive experiments based on real data sets demonstrate the efficiency of our proposed schema discovery algorithms, and practical use cases in real-world data exploration and integration scenarios are presented to illustrate the effective-ness of using skeletons in these applications.},
author = {Wang, Lanjun and Hassanzadeh, Oktie and Zhang, Shuo and Shi, Juwei and Jiao, Limei and Zou, Jia and Wang, Chen},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Schema Management for Document Stores.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Schema Management for Document Stores}},
url = {http://delivery.acm.org/10.1145/2780000/2777601/p922-wang.pdf?ip=137.226.149.66{\&}id=2777601{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=811457292{\&}CFTOKEN=71167132{\&}{\_}{\_}acm{\_}{\_}=1505898584{\_}1dbf180ccf0c50587a9896}
}
@article{Deng,
abstract = {Determining if two sets are related – that is, if they have similar val-ues or if one set contains the other – is an important problem with many applications in data cleaning, data integration, and informa-tion retrieval. For example, set relatedness can be a useful tool to discover whether columns from two different databases are join-able; if enough of the values in the columns match, it may make sense to join them. A particularly popular metric that has been proposed is to measure the relatedness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require exact matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the met-ric suffers from expensive computational cost, taking O(n 3) time, where n is the number of elements in sets, for each set-to-set com-parison. Thus for applications which try to search for all pairings of related sets in a brute-force manner, the runtime becomes unac-ceptably large. To address this challenge, we developed SILKMOTH, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SILKMOTH creates a signature for each set, with the property that any other set which is related must match the sig-nature. SILKMOTH then uses these signatures to prune the search space, so only sets which match the signatures are left as candi-dates. Finally, SILKMOTH applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. An important property of SILKMOTH is that it is guaranteed to output the exact same related set pairings as the brute-force method, allowing for consistency in results. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on in-sights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verifica-tion. In addition, we introduce a simple optimization to the calcu-lation of the maximum matching metric itself based on the triangle inequality. Compared to related approaches, SILKMOTH is much more general, handling a larger space of similarity functions and relatedness metrics, and is an order of magnitude more efficient on real datasets.},
author = {Deng, Dong and Kim, Albert and Madden, Samuel and Stonebraker, Michael},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - SILKMOTH An Efficient Method for Finding Related Sets with Maximum Matching Constraints.pdf:pdf},
mendeley-groups = {i5 - literature search},
number = {10},
title = {{SILKMOTH: An Efficient Method for Finding Related Sets with Maximum Matching Constraints}},
url = {https://arxiv.org/pdf/1704.04738.pdf},
volume = {10}
}
@article{Golshan,
abstract = {The field of data integration has expanded significantly over the years, from providing a uniform query and update interface to struc-tured databases within an enterprise to the ability to search, ex-change, and even update, structured or unstructured data that are within or external to the enterprise. This paper describes the evolution in the landscape of data inte-gration since the work on rewriting queries using views in the mid-1990's. In addition, we describe two important challenges for the field going forward. The first challenge is to develop good open-source tools for different components of data integration pipelines. The second challenge is to provide practitioners with viable solu-tions for the long-standing problem of systematically combining structured and unstructured data.},
author = {Golshan, Behzad and Halevy, Alon and Mihaila, George and Tan, Wang-Chiew},
doi = {10.1145/3034786.3056124},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golshan et al. - Unknown - Data Integration After the Teenage Years.pdf:pdf},
keywords = {data integration,open-source,structured and unstructured data,views},
mendeley-groups = {i5 - literature search},
title = {{Data Integration: After the Teenage Years}},
url = {http://delivery.acm.org/10.1145/3060000/3056124/p101-golshan.pdf?ip=137.226.149.66{\&}id=3056124{\&}acc=ACTIVE SERVICE{\&}key=575DA4752A380C0F.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=811457292{\&}CFTOKEN=71167132{\&}{\_}{\_}acm{\_}{\_}=1505898356{\_}0cc6dfb4ba9b641a09c}
}
@article{Chen2005,
author = {Chen, Ding-yi and Li, Xue and Dong, Zhao Yang and Chen, Xia},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2005 - Effectiveness of Document Representation for Classification.pdf:pdf},
issn = {03029743},
mendeley-groups = {i5 - literature search},
pages = {368--377},
title = {{Effectiveness of Document Representation for Classification}},
year = {2005}
}
@article{Kumar2012,
author = {Kumar, A Anil and Chandrasekhar, S.},
journal = {International Journal of Engineering Research {\&} Technology (IJERT)},
mendeley-groups = {i5 - literature search},
number = {5},
pages = {1--6},
title = {{Text Data Pre-processing and Dimensionality Reduction Techniques for Document Clustering}},
volume = {1},
year = {2012}
}
@article{Lakshminarayan2005,
abstract = {Improving customer experience on company web sites is an important aspect of maintaining a competitive edge in the technology industry. To better understand customer behavior, e-commerce sites provide online surveys for individual web site visitors to record their feedback with site performance. This paper describes some areas where text mining appears to have useful applications. For comments from web site visitors, we implemented automated analysis to discover emerging problems on the web site using clustering methods and furthermore devised procedures to assign comments to pre-defined categories using statistical classification. Statistical clustering was based on a Gaussian mixture model and hierarchical clustering to uncover new issues related to customer care-abouts. Statistical classification of comments was studied extensively by applying a variety of popular algorithms. We benchmarked their performance and make some recommendations based on our evaluations.},
author = {Lakshminarayan, C and Yu, Q F and Benson, A},
isbn = {0302-9743},
journal = {Databases in Networked Information Systems, Proceedings},
keywords = {categorization},
mendeley-groups = {i5 - literature search},
pages = {288--299},
title = {{Improving customer experience via text mining}},
volume = {3433},
year = {2005}
}
@article{Wang,
abstract = {Document classification presents difficult challenges due to the sparsity and the high dimensionality of text data, and to the complex semantics of the natural language. The tradi-tional document representation is a word-based vector (Bag of Words, or BOW), where each dimension is associated with a term of the dictionary containing all the words that ap-pear in the corpus. Although simple and commonly used, this representation has several limitations. It is essential to embed semantic information and conceptual patterns in or-der to enhance the prediction capabilities of classification algorithms. In this paper, we overcome the shortages of the BOW approach by embedding background knowledge de-rived from Wikipedia into a semantic kernel, which is then used to enrich the representation of documents. Our empir-ical evaluation with real data sets demonstrates that our ap-proach successfully achieves improved classification accuracy with respect to the BOW technique, and to other recently developed methods.},
author = {Wang, Pu and Domeniconi, Carlotta},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Domeniconi - Unknown - Building Semantic Kernels for Text Classification using Wikipedia.pdf:pdf},
keywords = {Clustering—algorithms,I70 [Document and Text Processing],Kernel Methods,Semantic Kernels,Wikipedia,sim-ilarity measure},
mendeley-groups = {i5 - literature search},
title = {{Building Semantic Kernels for Text Classification using Wikipedia}},
url = {https://pdfs.semanticscholar.org/d91a/d140009edc775959df3c57c4be5c8793d063.pdf}
}
@article{MinTjoa,
author = {{Min Tjoa}, A and Trujillo, Juan},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Min Tjoa, Trujillo - Unknown - LNCS 3589 - Data Warehousing and Knowledge Discovery.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{LNCS 3589 - Data Warehousing and Knowledge Discovery}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F11546849.pdf}
}
@article{YongguangBao2002,
abstract = {The basic k-nearest neighbor classifier works well in text classification. However, improving performance of the classifier is still attractive. Combining multiple classifiers is an effective technique for improving accuracy. There are many general combining algorithms, such as Bagging, or Boosting that significantly improve the classifier such as decision trees, rule learners, or neural networks. Unfortunately, these combining methods do not improve the nearest neighbor classifiers. In this paper we present a new approach to general multiple reducts based on rough sets theory, in which we apply multiple reducts to improve the performance of the k-nearest neighbor classifier. This paper describes the proposed technique and provides experimental results.

},
author = {{Yongguang Bao} and {Naohiro Ishii}},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yongguang Bao, Naohiro Ishii - 2002 - Combining Multiple K-Nearest Neighbor Classifiers for Text Classification by Reducts.pdf:pdf},
journal = {5th International conference, Data Science},
mendeley-groups = {i5 - literature search},
title = {{Combining Multiple K-Nearest Neighbor Classifiers for Text Classification by Reducts}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F3-540-36182-0.pdf},
year = {2002}
}
@article{Lee,
abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multi-plicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary func-tion analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diag-onally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.},
author = {Lee, Daniel D and Seung, H Sebastian},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seung - Unknown - Algorithms for Non-negative Matrix Factorization.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Algorithms for Non-negative Matrix Factorization}},
url = {https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf}
}
@article{Fuka,
author = {Fuka, Karel},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuka - Unknown - Feature Set Reduction for Document Classification Problems.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{Feature Set Reduction for Document Classification Problems}}
}
@article{Kadhim,
abstract = {-Text mining defines generally the process of extracting interesting features (non-trivial) and knowledge from unstructured text documents. Text mining is an interdisciplinary field which depends on information retrieval, data mining, machine learning, parameter statistics and computational linguistics. Standard text mining and retrieval information techniques of text document usually rely on similar categories. An alternative method of retrieving information is clustering documents to preprocess text. The preprocessing steps have a huge effect on the success to extract knowledge. This study implements TF-IDF and singular value decomposition (SVD) dimensionality reduction techniques. The proposed system presents an effective preprocessing and dimensionality reduction techniques which help the document clustering by using k-means algorithm. Finally, the experimental results show that the proposed method enhances the performance of English text document clustering. Simulation results on BBC news and BBC sport datasets show the superiority of the proposed algorithm.},
author = {Kadhim, Ammar Ismael and Cheah, Yu-N and Ahamed, Nurul Hashimah},
doi = {10.1109/ICAIET.2014.21},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kadhim, Cheah, Ahamed - Unknown - Text Document Preprocessing and Dimension Reduction Techniques for Text Document Clustering.pdf:pdf},
keywords = {-Text mining,clustering,dimension reduction,k-means,retrieval information,singular value decomposition},
mendeley-groups = {i5 - literature search},
title = {{Text Document Preprocessing and Dimension Reduction Techniques for Text Document Clustering}},
url = {http://uksim.info/icaiet2014/CD/data/7910a069.pdf}
}
@article{Ponmuthuramalingam2010,
abstract = {Frequent term based text clustering is a text clustering technique, which uses frequent term set and dramatically decreases the dimensionality of the document vector space, thus especially addressing: very high dimensionality of the data and very large size of the databases. Frequent Term based Clustering algorithm (FTC) has shown significant efficiency comparing to some well known text clustering methods, but the quality of clustering still needs further enhancement. In this paper, the morphological variant words, stopwords and grammatical words are identified and removed for further dimension reduction. Two effective dimension reduction algorithms, improved stemming and frequent term generation algorithms have been presented. An experiment on classical text documents as well as on web documents demonstrates that the developed algorithms yield good dimension reduction.},
author = {Ponmuthuramalingam, P and Devi, T},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponmuthuramalingam, Devi - 2010 - Effective Dimension Reduction Techniques for Text Documents.pdf:pdf},
journal = {IJCSNS International Journal of Computer Science and Network Security},
keywords = {Dimension reduction,Information retrieval,Latent semantic,Text documents,Text representation},
mendeley-groups = {i5 - literature search},
number = {7},
title = {{Effective Dimension Reduction Techniques for Text Documents}},
url = {http://paper.ijcsns.org/07{\_}book/201007/20100712.pdf},
volume = {10},
year = {2010}
}
@article{Jacoby2014,
author = {Jacoby, William G and Ciuk, David J},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacoby, Ciuk - 2014 - MULTIDIMENSIONAL SCALING AN INTRODUCTION.pdf:pdf},
mendeley-groups = {i5 - literature search},
title = {{MULTIDIMENSIONAL SCALING: AN INTRODUCTION}},
url = {http://polisci.msu.edu/jacoby/research/scaling/intromds/Jacoby-Ciuk, MDS, V2, 10-29-14.pdf},
year = {2014}
}
@article{Lodhi2002,
abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a stan-dard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets.},
author = {Lodhi, Huma and Saunders, Craig and Shawe-Taylor, John and Cristianini, Nello and Watkins, Chris},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lodhi et al. - 2002 - Text Classification using String Kernels.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Approx-imating Kernels,Kernels and Support Vector Machines,String Subsequence Kernel,Text Classification},
mendeley-groups = {i5 - literature search},
pages = {419--444},
title = {{Text Classification using String Kernels}},
url = {http://www.jmlr.org/papers/volume2/lodhi02a/lodhi02a.pdf},
volume = {2},
year = {2002}
}
@article{Xu2017,
abstract = {Short text clustering is a challenging problem due to its sparseness of text rep-resentation. Here we propose a flexible Self-Taught Convolutional neural net-work framework for Short Text Clustering (dubbed STC 2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Exten-sive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.00185v1},
author = {Xu, Jiaming and Xu, Bo and Wang, Peng and Zheng, Suncong and Tian, Guanhua and Zhao, Jun},
eprint = {arXiv:1701.00185v1},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2017 - Self-Taught Convolutional Neural Networks for Short Text Clustering.pdf:pdf},
keywords = {Neural Networks,Semantic Clustering,Short Text,Unsupervised Learning},
mendeley-groups = {i5 - literature search},
title = {{Self-Taught Convolutional Neural Networks for Short Text Clustering}},
url = {https://arxiv.org/pdf/1701.00185.pdf},
year = {2017}
}
@article{Xu,
abstract = {Short text clustering has become an increas-ing important task with the popularity of so-cial media, and it is a challenging problem due to its sparseness of text representation. In this paper, we propose a Short Text Clustering via Convolutional neural networks (abbr. to STCC), which is more beneficial for cluster-ing by considering one constraint on learned features through a self-taught learning frame-work without using any external tags/labels. First, we embed the original keyword features into compact binary codes with a locality-preserving constraint. Then, word embed-dings are explored and fed into convolution-al neural networks to learn deep feature rep-resentations, with the output units fitting the pre-trained binary code in the training pro-cess. After obtaining the learned representa-tions, we use K-means to cluster them. Our extensive experimental study on two public short text datasets shows that the deep fea-ture representation learned by our approach can achieve a significantly better performance than some other existing features, such as term frequency-inverse document frequency, Laplacian eigenvectors and average embed-ding, for clustering.},
author = {Xu, Jiaming and Wang, Peng and Tian, Guanhua and Xu, Bo and Zhao, Jun and Wang, Fangyuan and Hao, Hongwei},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - Unknown - Short Text Clustering via Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {i5 - literature search},
pages = {62--69},
title = {{Short Text Clustering via Convolutional Neural Networks}},
url = {http://www.aclweb.org/anthology/W15-1509}
}
@article{Mcdonald,
abstract = {Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organiza-tions, some people assist others in locating expertise by making referrals. People who make referrals fill key organ-izational roles that have been identified by CSCW and af-filiated research. Expertise locating systems are not de-signed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to de-crease workload and support people who have no other options. Recommendation systems are collaborative soft-ware that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our exper-tise recommendation system details the work necessary to fit expertise recommendation to a work setting. The archi-tecture and implementation begin to tease apart the techni-cal aspects of providing good recommendations from social and collaborative concerns.},
author = {Mcdonald, David W and Ackerman, Mark S},
file = {:C$\backslash$:/Users/Atul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcdonald, Ackerman - Unknown - Expertise Recommender A Flexible Recommendation System and Architecture.pdf:pdf},
keywords = {CSCW,Recommendation systems,collaborative filtering,computer-supported cooperative work,expert locators,expertise finding,expertise location,information seeking},
mendeley-groups = {i5 - literature search},
title = {{Expertise Recommender: A Flexible Recommendation System and Architecture}}
}
